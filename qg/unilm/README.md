# Answer Aware Question Generation Model

## Setup
Follow the steps mentioned at: https://github.com/microsoft/unilm

In order to run inference on CPU, please read these steps before installing UniLM.

* Do not install nvidia/apex module
* Do not use -fp16 -amp flags in the python decode_seq2seq command options
https://github.com/microsoft/unilm/issues/23

* Filename: code/unilm/src/run_finetuned_custom.sh
-python2 qg/eval_on_unilm_tokenized_ref.py --out_file qg/output/qg.test.output.txt
-python2 qg/eval.py --out_file qg/output/qg.test.output.txt

* Filename: code/unilm/src/pytorch_pretrained_bert/__init__.py
-from .tokenization import BertTokenizer, BasicTokenizer, WordpieceTokenizer
+from .tokenization import BertTokenizer, BasicTokenizer, WordpieceTokenizer, WhitespaceTokenizer

* python setup.py install --user

* Filename: code/unilm/src/biunilm/seq2seq_decoder.py
-model_recover = torch.load(model_recover_path)
+model_recover = torch.load(model_recover_path, map_location=torch.device('cpu'))


## Data

* data/validation_50.json - This file contains a set of 133 paragraphs, with each paragraph containing a single question, in SQuAD format. We manually analyze over 400 paragraphs and filter out the above 133 questions to analyze the quality of the generated questions. Please use this file to analyze the type of questions generated by the UniLM model.
* data/unilm-languagechecker-4930pqa.json - Contains 4930 questions generated by our QA Dataset Greneration Framework (after passing through language checker)
* data/augmented-squad-sar-unilm-languagechecker-4930pqa.json - Contains SQuAD training data merged with our generated <P, Q, A> triplets

## Scripts

This folder contains alist of utility scripts.

* unilm_out_to_squad.py: Converts unilm output to SQuAD format
* merge_squad_files.py: Merges 2 files in SQuAD formats
* qa_to_unilm.py: Converts a specific <passage, question, answer> format to unilm input format

## run_finetuned_custom.sh
This file runs the UniLM model for any given input present in the test folder.

## Running Steps:

* conda activate unilm
* cd code/unilm/src
* ./run_finetuned_custom.sh
